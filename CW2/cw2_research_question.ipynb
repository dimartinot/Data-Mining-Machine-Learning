{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#F21DL-Data-Mining-&amp;-Machine-Learning\" data-toc-modified-id=\"F21DL-Data-Mining-&amp;-Machine-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>F21DL Data Mining &amp; Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coursework-2-:--Convolutional-Network-for-Image-Classification-with-Transfer-Learning\" data-toc-modified-id=\"Coursework-2-:--Convolutional-Network-for-Image-Classification-with-Transfer-Learning-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Coursework 2 :  Convolutional Network for Image Classification with Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-constants\" data-toc-modified-id=\"Imports-&amp;-constants-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Imports &amp; constants</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F21DL Data Mining & Machine Learning\n",
    "\n",
    "## Coursework 2 :  Convolutional Network for Image Classification with Transfer Learning\n",
    "---\n",
    "*ACCAD Dimitri, AUZIMOUR Antoine, DELTEL Clarence, DI MARTINO Thomas*\n",
    "\n",
    "This notebook presents our work in the research question. \n",
    "In our research work, we wanted to benchmark the performances of a more common Deep Learning algorithm used to deal with data that have an architecture in it, whether it is spatial or temporal: the convolutional neural networks.\n",
    "\n",
    "<figure style=\"margin-top, margin-bottom:10px; text-align: center\">\n",
    "        <img src=\"https://www.learnopencv.com/wp-content/uploads/2017/11/convolution-example-matrix.gif\">\n",
    "    <figcaption style=\"display:block;margin-left:auto;margin-right:auto\"><u>Animation of the convolutional process (source: <a href=\"https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/\">link</a>)</u></figcaption>\n",
    "</figure>\n",
    "\n",
    "These neural networks use the process of convolution (c.f. above) to generate <b>features maps</b> that are dense in information and that can help a classifier to have more ease to find the correct class.\n",
    "\n",
    "This process of extracting features from images is transferable to a lot of different tasks: given a dataset where we want to discriminate cats from dogs, cars from motorcycles, a CNN will always look for patterns, more or less complex, depending on the depth of the network, that makeup any object (it could be a line, a curve, a round, gaps in colors...). This is the combination of these objects that leads a CNN to detect a given object in an image. \n",
    "Hence, the first part, consisting in finding the said patterns is easily transferable from one task to another: this famous and common process is called \"**Transfer Learning**\".\n",
    "</br>\n",
    "<figure style=\"margin-top: 10px;margin-bottom: 10px;text-align:center\">\n",
    "        <img src=\"https://ruder.io/content/images/2017/03/transfer_learning_setup.png\">\n",
    "    <figcaption><u>Illustration of the general concept of Transfer Learning (source: <a href=\"https://ruder.io/transfer-learning/\">link</a>)</u></figcaption>\n",
    "    \n",
    "</figure>\n",
    "\n",
    "For our research question, we will use the **VGG16 architecture** to detect the class of our streetsigns.\n",
    "<figure style=\"margin-top: 10px;margin-bottom: 10px; text-align:center\">\n",
    "    <img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/googlenet.png\">\n",
    "    <figcaption><u>Illustration of the architecture of the Inception network (source: <a href=\"https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/\">link</a>)</u></figcaption>\n",
    "    \n",
    "</figure>\n",
    "\n",
    "We can here see that the VGG16 architecture is made of different part:\n",
    " - A convolutional part used to extract features from the input image;\n",
    " - A fully-connected part used to classify the extracted features to adequate classes.\n",
    " \n",
    "In our situation, we will used the VGG16 that was trained on the '*imagenet*' dataset. However, in this dataset, 1000 classes are used to make predictions: hence, we will not keep the fully connected layers of the original network and we will build our own classifier on top of it to predict our 10 classes of panels. This process of scrapping away the connected layers from a network to only keep the convolutional part is very common when applying transfer learning.\n",
    "\n",
    "Now for the code part !\n",
    "\n",
    "### Imports & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MAIN IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "# KERAS IMPORTS\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Conv2D, Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# SKLEARN IMPORTS\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "ORIGINAL_IMAGE_SHAPE = (48, 48)\n",
    "VGG_IMAGE_SHAPE = (224,224)\n",
    "NUM_CLASSES = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42\n",
    "IMG_FOLDER = \"data\"\n",
    "ROOT_FOLDER = os.getcwd()\n",
    "NB_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier for Keras to work with the images, we will save all of them as real .png file in a data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_csv_to_disk(PATH_TO_CSV=\"x_train_gr_smpl.csv\", PATH_TO_DISK=\"data\"):\n",
    "    print(\"Opening csv..\")\n",
    "    data = pd.read_csv(PATH_TO_CSV)\n",
    "    print(\"Csv opened..\")\n",
    "    print(\"Saving images to {}\".format(os.path.join(os.getcwd(),PATH_TO_DISK)))\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        np_row = row.to_numpy()\n",
    "        img = np_row.reshape(ORIGINAL_IMAGE_SHAPE).astype(np.float32)\n",
    "        cv2_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.imwrite(\n",
    "            os.path.join(os.path.join(os.getcwd(),PATH_TO_DISK), f\"img{index}.png\"),#path to img\n",
    "            cv2_img\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from_csv_to_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the useful classes for our problem:\n",
    " - The model class that we will use to interact with our model\n",
    " - The Generator Keras object that will provide data to our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(dataframe, path_column_name=\"imgpath\", class_column_name=\"class\", is_test = False):\n",
    "    \"\"\"\n",
    "    Dataframe should contain two columns:\n",
    "     - img path column (default name: 'imgpath')\n",
    "     - img class column (default name: 'class')\n",
    "     \n",
    "    If is_test: return a single generator\n",
    "    If is_test is False: return a tuple of train and val datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if (is_test):\n",
    "        \n",
    "        generator = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        test_generator = generator.flow_from_dataframe(\n",
    "            directory=os.path.join(ROOT_FOLDER, IMG_FOLDER),\n",
    "            dataframe=dataframe,  \n",
    "            x_col=path_column_name, \n",
    "            y_col=class_column_name, \n",
    "            class_mode=\"categorical\", \n",
    "            target_size=VGG_IMAGE_SHAPE,\n",
    "            shuffle=False,\n",
    "            color_mode=\"rgb\", #the last two parameters induce the resizing to (224,224,3)\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        return test_generator\n",
    "    \n",
    "    else:      \n",
    "    \n",
    "        generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2, # set validation split\n",
    "            #data augmentation\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "        train_generator = generator.flow_from_dataframe(\n",
    "            directory=os.path.join(ROOT_FOLDER, IMG_FOLDER),\n",
    "            dataframe=dataframe,  \n",
    "            x_col=path_column_name, \n",
    "            y_col=class_column_name, \n",
    "            class_mode=\"categorical\", \n",
    "            target_size=VGG_IMAGE_SHAPE,\n",
    "            color_mode=\"rgb\", #the last two parameters induce the resizing to (224,224,3)\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=SEED,\n",
    "            subset='training') \n",
    "\n",
    "        validation_generator = generator.flow_from_dataframe(\n",
    "            directory=os.path.join(ROOT_FOLDER, IMG_FOLDER),\n",
    "            dataframe=dataframe,  \n",
    "            x_col=path_column_name, \n",
    "            y_col=class_column_name, \n",
    "            class_mode=\"categorical\", \n",
    "            target_size=VGG_IMAGE_SHAPE,\n",
    "            color_mode=\"rgb\", #the last two parameters induce the resizing to (224,224,3)\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=SEED,\n",
    "            subset='validation') \n",
    "    \n",
    "        return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedCNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        \n",
    "        x = Dense(1024, activation=\"relu\")(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(256, activation=\"relu\")(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "        \n",
    "        \n",
    "        self.model = Model(inputs= base_model.input, outputs=x)\n",
    "        \n",
    "        opt = Adam(learning_rate=LR)\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "    def train(self, datagen):\n",
    "        \"\"\"\n",
    "        Make sure to create the datagen object with the 'get_generator' method: it should be a tuple of training and validation generators\n",
    "        \"\"\"\n",
    "        train_gen, val_gen = datagen\n",
    "        \n",
    "        filepath=\"toplayer_training-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "        \n",
    "        return self.model.fit_generator(    \n",
    "            train_gen,\n",
    "            epochs=NB_EPOCHS,\n",
    "            steps_per_epoch = train_gen.samples,\n",
    "            validation_data = val_gen, \n",
    "            validation_steps = val_gen.samples,\n",
    "            verbose=0, \n",
    "            callbacks=[\n",
    "                TQDMNotebookCallback(), \n",
    "                checkpoint,\n",
    "                EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def finetune(self,datagen):\n",
    "        \"\"\"\n",
    "        Make sure to create the datagen object with the 'get_generator' method: it should be a tuple of training and validation generators\n",
    "        \"\"\"\n",
    "        train_gen, val_gen = datagen\n",
    "        \n",
    "        filepath=\"finetune_training-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "        \n",
    "        for layer in self.model.layers[:249]:\n",
    "            layer.trainable = False\n",
    "        for layer in self.model.layers[249:]:\n",
    "            layer.trainable = True\n",
    "            \n",
    "        opt = Adam(learning_rate=LR)\n",
    "\n",
    "        self.model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "        # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "        # alongside the top Dense layers\n",
    "        return self.model.fit_generator(\n",
    "            train_gen,\n",
    "            epochs=NB_EPOCHS,\n",
    "            steps_per_epoch = train_gen.samples,\n",
    "            validation_data = val_gen, \n",
    "            validation_steps = val_gen.samples,\n",
    "            verbose=0, \n",
    "            callbacks=[\n",
    "                TQDMNotebookCallback(), \n",
    "                checkpoint,\n",
    "                EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.025)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def test(self, datagen, steps=1):\n",
    "        \"\"\"\n",
    "        Make sure to create the datagen object with the 'get_generator' method\n",
    "        \"\"\"\n",
    "        return self.model.evaluate_generator(datagen, steps=steps)\n",
    "    \n",
    "    def predict(self, datagen):\n",
    "        \"\"\"\n",
    "        Make sure to create the datagen object with the 'get_generator' method        \n",
    "        \"\"\"\n",
    "        return self.model.predict_generator(datagen)\n",
    "    \n",
    "    def load_from_fileweights(self, path):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model.load_weights(os.path.join(os.getcwd(), path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load data and launching training sessions using the same 10-fold validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = np.array(\n",
    "                sorted(os.listdir(os.path.join(ROOT_FOLDER, IMG_FOLDER)), \n",
    "                     key=lambda x: eval(x[3:][:-4])# we sort images with their index number\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"y_train_smpl.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgpath</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>img0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>img1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>img2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>img3.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>img4.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imgpath class\n",
       "0  img0.png     0\n",
       "1  img1.png     0\n",
       "2  img2.png     0\n",
       "3  img3.png     0\n",
       "4  img4.png     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = pd.DataFrame(classes)\n",
    "c = c.astype(str)\n",
    "df = pd.concat([pd.DataFrame(np.expand_dims(images_path, axis=1)), c], axis=1)\n",
    "df.columns = [\"imgpath\", \"class\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe has now the correct information, we can create the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8103 validated image filenames belonging to 10 classes.\n",
      "Found 2025 validated image filenames belonging to 10 classes.\n",
      "Found 2532 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"imgpath\"], df[\"class\"], test_size=0.20, random_state=SEED)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "#with tf.device('/cpu:0'):\n",
    "model = PreTrainedCNN()\n",
    "\n",
    "datagen_train = get_generator(train_df) # tuple (train_gen, val_ge)\n",
    "datagen_test = get_generator(test_df, is_test=True) # single generator for test\n",
    "\n",
    "model.load_from_fileweights(\"toplayer_training-02-0.70.hdf5\")\n",
    "\n",
    "#hist = model.train(datagen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTested metrics are: ['loss', 'accuracy']\n",
      "\t\tPerf:  [1.400138258934021, 0.75]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\tTested metrics are:\", model.model.metrics_names)\n",
    "print(\"\\t\\tPerf: \",model.test(datagen_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just tested our network after finetuning the top fully connected layer. We will now see its performance when launching a training session on its last 250 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_bis = model.finetune(datagen_test)\n",
    "model.load_from_fileweights(\"finetune_0.875_acc_test_set.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTested metrics are: ['loss', 'accuracy']\n",
      "\t\tPerf:  [0.4105914235115051, 0.875]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\tTested metrics are:\", model.model.metrics_names)\n",
    "print(\"\\t\\tPerf: \",model.test(datagen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(datagen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.355641</td>\n",
       "      <td>0.635838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.786948</td>\n",
       "      <td>0.997691</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755134</td>\n",
       "      <td>0.752007</td>\n",
       "      <td>0.775413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>0.284974</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.997567</td>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.969863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755134</td>\n",
       "      <td>0.633498</td>\n",
       "      <td>0.755134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.459259</td>\n",
       "      <td>0.393560</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.951267</td>\n",
       "      <td>0.879828</td>\n",
       "      <td>0.962138</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.863415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755134</td>\n",
       "      <td>0.638029</td>\n",
       "      <td>0.733179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>support</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.755134</td>\n",
       "      <td>2532.000000</td>\n",
       "      <td>2532.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1          2           3           4  \\\n",
       "precision    0.355641    0.635838   1.000000    0.979920    0.786948   \n",
       "recall       0.648084    0.284974   0.114943    0.924242    0.997567   \n",
       "f1-score     0.459259    0.393560   0.206186    0.951267    0.879828   \n",
       "support    287.000000  386.000000  87.000000  264.000000  411.000000   \n",
       "\n",
       "                    5           6          7           8     9  accuracy  \\\n",
       "precision    0.997691    0.986014   1.000000    0.778022   0.0  0.755134   \n",
       "recall       0.929032    0.898089   0.568182    0.969863   0.0  0.755134   \n",
       "f1-score     0.962138    0.940000   0.724638    0.863415   0.0  0.755134   \n",
       "support    465.000000  157.000000  44.000000  365.000000  66.0  0.755134   \n",
       "\n",
       "             macro avg  weighted avg  \n",
       "precision     0.752007      0.775413  \n",
       "recall        0.633498      0.755134  \n",
       "f1-score      0.638029      0.733179  \n",
       "support    2532.000000   2532.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.classification_report(y_test,np.argmax(y_pred, axis=1).astype(str) ,  output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
